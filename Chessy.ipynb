{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c2fe274-8221-40d3-b28b-42752ae54352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e2e4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stockfish import Stockfish\n",
    "stockfish = Stockfish('stockfish-windows-x86-64-avx2.exe')\n",
    "stockfish.set_fen_position(\"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\")\n",
    "\n",
    "stockfish.get_best_move()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56f76173-3117-4593-967e-0a5d92ba3919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id name Lc0 v0.31.2\n",
      "id author The LCZero Authors.\n",
      "option name WeightsFile type string default <autodiscover>\n",
      "option name Backend type combo default cuda-auto var cuda-auto var cuda var cuda-fp16 var trivial var random var check var recordreplay var roundrobin var multiplexing var demux\n",
      "option name BackendOptions type string default\n",
      "option name Threads type spin default 0 min 0 max 128\n",
      "option name NNCacheSize type spin default 2000000 min 0 max 999999999\n",
      "option name MinibatchSize type spin default 0 min 0 max 1024\n",
      "option name MaxPrefetch type spin default 32 min 0 max 1024\n",
      "option name CPuct type string default 1.745000\n",
      "option name CPuctBase type string default 38739.000000\n",
      "option name CPuctFactor type string default 3.894000\n",
      "option name TwoFoldDraws type check default true\n",
      "option name VerboseMoveStats type check default false\n",
      "option name FpuStrategy type combo default reduction var reduction var absolute\n",
      "option name FpuValue type string default 0.330000\n",
      "option name CacheHistoryLength type spin default 0 min 0 max 7\n",
      "option name PolicyTemperature type string default 1.359000\n",
      "option name MaxCollisionEvents type spin default 917 min 1 max 65536\n",
      "option name MaxCollisionVisits type spin default 80000 min 1 max 100000000\n",
      "option name MaxCollisionVisitsScalingStart type spin default 28 min 1 max 100000\n",
      "option name MaxCollisionVisitsScalingEnd type spin default 145000 min 0 max 100000000\n",
      "option name MaxCollisionVisitsScalingPower type string default 1.250000\n",
      "option name OutOfOrderEval type check default true\n",
      "option name MaxOutOfOrderEvalsFactor type string default 2.400000\n",
      "option name StickyEndgames type check default true\n",
      "option name SyzygyFastPlay type check default false\n",
      "option name MultiPV type spin default 1 min 1 max 500\n",
      "option name PerPVCounters type check default false\n",
      "option name ScoreType type combo default WDL_mu var centipawn var centipawn_with_drawscore var centipawn_2019 var centipawn_2018 var win_percentage var Q var W-L var WDL_mu\n",
      "option name HistoryFill type combo default fen_only var no var fen_only var always\n",
      "option name MovesLeftMaxEffect type string default 0.034500\n",
      "option name MovesLeftThreshold type string default 0.800000\n",
      "option name MovesLeftSlope type string default 0.002700\n",
      "option name MovesLeftConstantFactor type string default 0.000000\n",
      "option name MovesLeftScaledFactor type string default 1.652100\n",
      "option name MovesLeftQuadraticFactor type string default -0.652100\n",
      "option name MaxConcurrentSearchers type spin default 1 min 0 max 128\n",
      "option name DrawScore type string default 0.000000\n",
      "option name ContemptMode type combo default play var play var white_side_analysis var black_side_analysis var disable\n",
      "option name Contempt type string default\n",
      "option name WDLCalibrationElo type string default 0.000000\n",
      "option name WDLEvalObjectivity type string default 1.000000\n",
      "option name WDLDrawRateReference type string default 0.500000\n",
      "option name NodesPerSecondLimit type string default 0.000000\n",
      "option name SolidTreeThreshold type spin default 100 min 1 max 2000000000\n",
      "option name TaskWorkers type spin default -1 min -1 max 128\n",
      "option name MinimumProcessingWork type spin default 20 min 2 max 100000\n",
      "option name MinimumPickingWork type spin default 1 min 1 max 100000\n",
      "option name MinimumRemainingPickingWork type spin default 20 min 0 max 100000\n",
      "option name MinimumPerTaskProcessing type spin default 8 min 1 max 100000\n",
      "option name IdlingMinimumWork type spin default 0 min 0 max 10000\n",
      "option name ThreadIdlingThreshold type spin default 1 min 0 max 128\n",
      "option name UCI_Opponent type string default\n",
      "option name UCI_RatingAdv type string default 0.000000\n",
      "option name SearchSpinBackoff type check default false\n",
      "option name ConfigFile type string default lc0.config\n",
      "option name SyzygyPath type string default\n",
      "option name Ponder type check default true\n",
      "option name UCI_Chess960 type check default false\n",
      "option name UCI_ShowWDL type check default false\n",
      "option name UCI_ShowMovesLeft type check default false\n",
      "option name SmartPruningFactor type string default 1.330000\n",
      "option name SmartPruningMinimumBatches type spin default 0 min 0 max 10000\n",
      "option name RamLimitMb type spin default 0 min 0 max 100000000\n",
      "option name MoveOverheadMs type spin default 200 min 0 max 100000000\n",
      "option name TimeManager type string default legacy\n",
      "option name ValueOnly type check default false\n",
      "option name LogFile type string default\n",
      "uciok\n",
      "info depth 1 seldepth 1 time 1038 nodes 1 score cp 49 tbhits 0 pv e2e4\n",
      "bestmove e2e4\n",
      "Le meilleur coup est : e2e4\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Chemins vers l'exécutable LCZero et le modèle\n",
    "LCZERO_PATH = \"Leila/lc0.exe\"  # Remplace par le chemin correct vers lc0.exe\n",
    " # Remplace par le chemin correct du modèle\n",
    "\n",
    "def get_best_move_lc0(fen_position):\n",
    "    # Commande pour exécuter lc0\n",
    "    command = [LCZERO_PATH]\n",
    "\n",
    "    # Utilisez subprocess pour lancer lc0 avec les entrées/sorties configurées\n",
    "    process = subprocess.Popen(command, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    \n",
    "    # Envoi de la position FEN et des commandes UCI à lc0\n",
    "    process.stdin.write(\"uci\\n\")                      # Initialisation du mode UCI\n",
    "    process.stdin.write(f\"position fen {fen_position}\\n\")  # Position de départ\n",
    "    process.stdin.write(\"go movetime 1000\\n\")         # Commande de recherche pour 1 seconde\n",
    "    process.stdin.flush()\n",
    "\n",
    "    # Lecture de la sortie pour obtenir le meilleur coup\n",
    "    best_move = None\n",
    "    for line in process.stdout:\n",
    "        print(line.strip())  # Affiche la sortie pour aider au débogage\n",
    "        if line.startswith(\"bestmove\"):\n",
    "            best_move = line.split()[1]\n",
    "            break\n",
    "    \n",
    "    # Fermer le processus\n",
    "    process.stdin.write(\"quit\\n\")\n",
    "    process.stdin.flush()\n",
    "    process.terminate()\n",
    "    \n",
    "    return best_move\n",
    "\n",
    "# Exemple d'utilisation\n",
    "fen = \"r1bqkbnr/pppppppp/n7/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\"\n",
    "best_move = get_best_move_lc0(fen)\n",
    "print(f\"Le meilleur coup est : {best_move}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b813be2-0fc0-4735-bdf7-14127254ec05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Utilisez votre token d'accès Hugging Face ici\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9f935c-a806-4b36-a73e-6f757f8cb1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2107: FutureWarning: Calling LlamaTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "# Définir le chemin vers le dossier où se trouvent vos fichiers\n",
    "model_path = \"C:/Users/Admin/.llama/checkpoints/Llama3.1-8B/consolidated.00.pth\"\n",
    "\n",
    "# Charger le tokenizer et le modèle\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "model = LlamaForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "# Exemple de texte à compléter ou générer\n",
    "input_text = \"Voici un exemple de texte pour le modèle Llama\"\n",
    "\n",
    "# Tokeniser le texte d'entrée\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Générer une réponse\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=100)\n",
    "\n",
    "# Décoder la réponse générée\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Afficher le texte généré\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dadeb611-80af-4d95-ac05-cde6f4cc4a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaa\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour obtenir une réponse concise\n",
    "def generate_flan_response(prompt, max_length=50):\n",
    "    inputs = tokenizer(\"Question: \" + prompt + \" Answer:\", return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs.input_ids, max_length=max_length)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Exemple de question\n",
    "question = \"Jouons au echec je te laisse commencer\"\n",
    "response = generate_flan_response(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c7646b-4e17-4fbf-a408-6c5dc7f166f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install SpeechRecognition pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d858080-293e-4869-b596-14cb8e175b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install SpeechRecognition pocketsphinx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fac765b-d2d3-4dda-9a49-789b2ff1b0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess \n",
    "board = chess.Board()\n",
    "board.push_san('e4')\n",
    "board.push_san('e5')\n",
    "print(board.fen())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a011e951-8903-4a4a-a4a1-944be28a9f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_moves = list(board.legal_moves)\n",
    "legal_moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "820aad42-3a01-4173-8a16-e1ff1c6abdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e68d749c-98a2-4447-8a01-c65c371b8299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "engine = pyttsx3.init()\n",
    "engine.say(stockfish.get_best_move())\n",
    "engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089445eb-db4b-4577-b8e9-167dfbc3882a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "\n",
    "# Initialisation\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Ajustement de la vitesse (par exemple, ralentir un peu pour rendre la voix plus naturelle)\n",
    "engine.setProperty('rate', 150)  # Par défaut, la vitesse est autour de 200\n",
    "\n",
    "# Ajustement du volume (entre 0.0 et 1.0)\n",
    "engine.setProperty('volume', 0.9)\n",
    "\n",
    "# Choisir une voix plus adaptée (si disponible)\n",
    "voices = engine.getProperty('voices')\n",
    "for voice in voices:\n",
    "    if \"french\" in voice.languages or \"FR\" in voice.id:\n",
    "        engine.setProperty('voice', voice.id)\n",
    "        break\n",
    "\n",
    "# Utiliser la voix pour une phrase de test\n",
    "engine.say(\"E\")\n",
    "engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "728439cb-bcdb-49bf-a8c2-3a7c8271e448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Talk\n",
      "Time over, thanks\n",
      "Text: je veux jouer aux échec\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "# Initialize recognizer class (for recognizing the speech)\n",
    "r = sr.Recognizer()\n",
    "\n",
    "# Reading Microphone as source\n",
    "# listening the speech and store in audio_text variable\n",
    "with sr.Microphone() as source:\n",
    "    print(\"Talk\")\n",
    "    audio_text = r.listen(source)\n",
    "    print(\"Time over, thanks\")\n",
    "    # recoginze_() method will throw a request\n",
    "    # error if the API is unreachable,\n",
    "    # hence using exception handling\n",
    "    \n",
    "    try:\n",
    "        # using google speech recognition\n",
    "        print(\"Text: \"+r.recognize_google(audio_text,language=\"fr-FR\"))\n",
    "    except:\n",
    "         print(\"Sorry, I did not get that\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abecad8a-d96c-4ba3-9207-ec446e589063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Utilisez votre token d'accès Hugging Face ici\n",
    "login(token=\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c632528-dbba-4670-b018-cc39c792fccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\"C:/Users/Admin/.llama/checkpoints/Llama3.2-1B/\")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"C:/Users/Admin/.llama/checkpoints/Llama3.2-1B/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51c1d9b8-c61b-4dda-b904-047ec9d8539e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me how to make bread : 100 recipes for artisan breads, from rustic to fancy, with step-by-step instructions and advice for making the best bread possible\n",
      "A comprehensive guide to making artisan bread at home, with recipes for 100 different types of bread, including baguettes, focaccia, sourdough, pizza dough, and more\n",
      "What is artisan bread? How does it differ from commercial bread? Why does artisan bread taste so good? How can I make it myself? These are the questions that will be answered in Tell Me How to Make Bread, the first cookbook to focus on artisan breads. With 100 recipes for breads that range from rustic to fancy, this book is the definitive guide to making artisan bread at home. Author and baker Elizabeth Karmel has created a cookbook that is both accessible and informative, with step-by-step instructions, tips, and advice for making the best bread possible. With recipes for everything from baguettes, focaccia, and sourdough to pizza dough and more, this book is an essential resource for anyone who wants to learn how to make artisan bread.\n",
      "Elizabeth Karmel is a baker and author of several cookbooks, including The Bread Baker's Apprentice and The Bread Baker's Apprentice Cookbook. She lives in New York City.\n",
      "\"Elizabeth Karmel's Tell Me How to Make Bread is a comprehensive, step-by-step guide to making artisan breads, from rustic to fancy, with recipes for 100 different types of bread, including baguettes, focaccia, sourdough, pizza dough, and more. With clear instructions and tips, Karmel demystifies artisan breadmaking, making it accessible to all. This is a must-have book for any baker's library.\" --Sara Moulton, host of public television's Sara Moulton Show and author of Sara's Secrets\n",
      "\"Elizabeth Karmel's Tell Me How to Make Bread is a comprehensive, step-by-step guide to making artisan breads, from rustic to fancy, with recipes for 100 different types of bread, including baguettes, focaccia, sourdough, pizza dough, and more. With clear instructions and tips, Karmel demystifies artisan breadmaking, making it accessible to all. This is a must-have book for any baker's library.\" --Sara Moulton, host of public television's Sara Moulton Show and author of Sara's\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Spécifiez le répertoire où vous avez sauvegardé votre modèle\n",
    "model_directory = \"C:/Users/Admin/.llama/checkpoints/Llama3.2-1B\"\n",
    "\n",
    "# Charger le tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "\n",
    "# Charger le modèle\n",
    "model = AutoModelForCausalLM.from_pretrained(model_directory)\n",
    "\n",
    "# Tester la tokenisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70e7a49c-45c8-4327-b02a-382be1aa92ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Joue aux echec de l'homme\\nJoue aux echec de l\"}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_id, \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "pipe(\"Joue aux echec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8f83037-a5e1-4c09-8690-510695935ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input length of input_ids is 40, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRegle de fonctionnement, si je te dis echec tu : dis blanc ou noir, si je te dis moteur de jeu tu dis : stockfish ou leila, echec :\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:272\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[1;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\base.py:1302\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1295\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1296\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1299\u001b[0m         )\n\u001b[0;32m   1300\u001b[0m     )\n\u001b[0;32m   1301\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\base.py:1309\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1308\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1309\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1310\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\base.py:1209\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1207\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1208\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1209\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1210\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:370\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[0;32m    368\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[1;32m--> 370\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    371\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:2068\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2065\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_supports_num_logits_to_keep() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_logits_to_keep\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[0;32m   2066\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_logits_to_keep\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 2068\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_generated_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_default_max_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2070\u001b[0m \u001b[38;5;66;03m# 7. Prepare the cache.\u001b[39;00m\n\u001b[0;32m   2071\u001b[0m \u001b[38;5;66;03m# - `model_kwargs` may be updated in place with a cache as defined by the parameters in `generation_config`.\u001b[39;00m\n\u001b[0;32m   2072\u001b[0m \u001b[38;5;66;03m# - different models have a different cache name expected by the model (default = \"past_key_values\")\u001b[39;00m\n\u001b[0;32m   2073\u001b[0m \u001b[38;5;66;03m# - `max_length`, prepared above, is used to determine the maximum cache length\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;66;03m# TODO (joao): remove `user_defined_cache` after v4.47 (remove default conversion to legacy format)\u001b[39;00m\n\u001b[0;32m   2075\u001b[0m cache_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmamba\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_params\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1383\u001b[0m, in \u001b[0;36mGenerationMixin._validate_generated_length\u001b[1;34m(self, generation_config, input_ids_length, has_default_max_length)\u001b[0m\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids_length \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[0;32m   1382\u001b[0m     input_ids_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1383\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1384\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput length of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but `max_length` is set to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This can lead to unexpected behavior. You should consider\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1386\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m increasing `max_length` or, better yet, setting `max_new_tokens`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1389\u001b[0m \u001b[38;5;66;03m# 2. Min length warnings due to unfeasible parameter combinations\u001b[39;00m\n\u001b[0;32m   1390\u001b[0m min_length_error_suffix \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1391\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Generation will stop at the defined maximum length. You should decrease the minimum length and/or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1392\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincrease the maximum length.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1393\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Input length of input_ids is 40, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`."
     ]
    }
   ],
   "source": [
    "pipe(\"Regle de fonctionnement, si je te dis echec tu : dis blanc ou noir, si je te dis moteur de jeu tu dis : stockfish ou leila, echec :\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e019036-f8a0-4a73-89bb-e05919a2343e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commence la partie d'echec! J'ai le sens de la victoire, mais j' n' ai pas la sens de la suite.\n",
      "Pierre, il n'y a pas de moyen de savoir comment le jeu va finir.\n",
      "C'était si étrange, il\n"
     ]
    }
   ],
   "source": [
    "response = pipe(\n",
    "    \"Commence la partie d'echec\",\n",
    "    max_new_tokens=50,  # Limite la longueur de la réponse générée\n",
    "    temperature=0.9,    # Rend les réponses plus créatives\n",
    "    top_p=0.95          # Filtrage pour encourager la diversité\n",
    ")\n",
    "\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c2c53a-487f-4818-b0fb-d7ce0618be5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
